{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import pandas as pd\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumericalData(soup):\n",
    "    release_year = getReleaseYear(soup)\n",
    "    duration = getDuration(soup)\n",
    "    average_rating = getAverageRating(soup)\n",
    "    rater_count = getRaterCount(soup)\n",
    "    fan_count = getFanCount(soup)\n",
    "    watch_count = getWatchCount(soup)\n",
    "    list_count = getListCount(soup)\n",
    "    like_count = getLikeCount(soup)\n",
    "    return {\n",
    "        \"Release Year\": release_year,\n",
    "        \"Duration\": duration,\n",
    "        \"Avg Rating\": average_rating,\n",
    "        \"Raters\": rater_count,\n",
    "        \"Fans\": fan_count,\n",
    "        \"Watched\": watch_count,\n",
    "        \"Lists\": list_count,\n",
    "        \"Likes\": like_count\n",
    "    }\n",
    "\n",
    "def getFanCount(soup):\n",
    "    fan_count_a = soup.find('a', class_='all-link more-link')\n",
    "    fan_count = fan_count_a.text if fan_count_a else 0\n",
    "    return fan_count\n",
    "\n",
    "def getLikeCount(soup):\n",
    "    like_count_li = soup.find('li', class_='stat filmstat-likes')\n",
    "    like_count_a = like_count_li.find('a', class_='has-icon') if like_count_li else None\n",
    "    like_count_text = like_count_a['data-original-title'] if like_count_a else None\n",
    "    like_count = like_count_text.split(\" \")[2] if like_count_text else \"N/A\"\n",
    "    return like_count\n",
    "\n",
    "def getListCount(soup):\n",
    "    list_count_li = soup.find('li', class_='stat filmstat-lists')\n",
    "    list_count_a = list_count_li.find('a', class_='has-icon') if list_count_li else None\n",
    "    list_count_text = list_count_a['data-original-title'] if list_count_a else None\n",
    "    list_count = list_count_text.split(\" \")[2] if list_count_text else \"N/A\"\n",
    "    return list_count\n",
    "\n",
    "def getWatchCount(soup):\n",
    "    watch_count_li = soup.find('li', class_='stat filmstat-watches')\n",
    "    watch_count_a = watch_count_li.find('a', class_='has-icon') if watch_count_li else None\n",
    "    watch_count_text = watch_count_a['data-original-title'] if watch_count_a else None\n",
    "    watch_count = watch_count_text.split(\" \")[2] if watch_count_text else \"N/A\"\n",
    "    return watch_count\n",
    "\n",
    "def getRaterCount(soup):\n",
    "    average_rating_span = soup.find('span', class_='average-rating')\n",
    "    average_rating_span_a = average_rating_span.find('a', class_='tooltip display-rating') if average_rating_span else None\n",
    "    if(average_rating_span_a == None):\n",
    "        average_rating_span_a = average_rating_span.find('a', class_='tooltip display-rating -highlight') if average_rating_span else None\n",
    "    average_rating_span_text = average_rating_span_a['data-original-title'] if average_rating_span_a else None\n",
    "    rater_count = average_rating_span_text.split(\" \")[6] if average_rating_span_text else \"N/A\"\n",
    "    return rater_count\n",
    "\n",
    "def getAverageRating(soup):\n",
    "    average_rating_span = soup.find('span', class_='average-rating')\n",
    "    average_rating_span_a = average_rating_span.find('a', class_='tooltip display-rating') if average_rating_span else None\n",
    "    if(average_rating_span_a == None):\n",
    "        average_rating_span_a = average_rating_span.find('a', class_='tooltip display-rating -highlight') if average_rating_span else None\n",
    "    average_rating_span_text = average_rating_span_a['data-original-title'] if average_rating_span_a else None\n",
    "    average_rating = average_rating_span_text.split(\" \")[3] if average_rating_span_text else \"N/A\"\n",
    "    return average_rating\n",
    "\n",
    "def getDuration(soup):\n",
    "    duration_p = soup.find('p', class_='text-link text-footer')\n",
    "    duration = duration_p.text.split(\" \")[0] if duration_p else \"N/A\"\n",
    "    return duration\n",
    "\n",
    "def getReleaseYear(soup):\n",
    "    release_year_div = soup.find('div', class_='releaseyear')\n",
    "    release_year = release_year_div.a.text if release_year_div else \"N/A\"\n",
    "    return release_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategoricalData(soup):\n",
    "    movie_name = getMovieTitle(soup)\n",
    "    genre = getGenre(soup)\n",
    "    details_divs = getDetailsDivs(soup)\n",
    "    sluglist_divs = getDetailsSlugList(details_divs)\n",
    "    details_headers = getDetailHeaders(details_divs)\n",
    "    studio = getStudio(sluglist_divs, details_headers) \n",
    "    country = getCountry(sluglist_divs, details_headers) \n",
    "    primary_language = getPrimaryLanguage(sluglist_divs, details_headers) \n",
    "    return {\n",
    "        \"Movie Name\": movie_name,\n",
    "        \"Genre\": genre,\n",
    "        \"Studio\": studio,\n",
    "        \"Country\": country,\n",
    "        \"Primary Language\": primary_language\n",
    "    }\n",
    "\n",
    "def getStudio(sluglist_divs, details_headers):\n",
    "    try:\n",
    "        if(details_headers[0] == \"Studios\" or details_headers[0] == \"Studio\"):\n",
    "            studio = sluglist_divs[0].find('a', class_='text-slug').text\n",
    "            return studio\n",
    "        return \"N/A\"\n",
    "    except IndexError:\n",
    "        return \"N/A\"\n",
    "\n",
    "def getCountry(sluglist_divs, details_headers):\n",
    "    try:\n",
    "        if(details_headers[0] == \"Countries\" or details_headers[0] == \"Country\"):\n",
    "            country = sluglist_divs[0].find('a', class_='text-slug').text\n",
    "            return country\n",
    "        elif(details_headers[1] == \"Countries\" or details_headers[1] == \"Country\"):\n",
    "            country = sluglist_divs[1].find('a', class_='text-slug').text\n",
    "            return country\n",
    "        return \"N/A\"\n",
    "    except IndexError:\n",
    "        return \"N/A\"\n",
    "\n",
    "def getPrimaryLanguage(sluglist_divs, details_headers):\n",
    "    try:\n",
    "        if(details_headers[0] == \"Primary Language\" or details_headers[0] == \"Language\"):\n",
    "            primary_language = sluglist_divs[0].find('a', class_='text-slug').text\n",
    "            return primary_language\n",
    "        elif(details_headers[1] == \"Primary Language\" or details_headers[1] == \"Language\"):\n",
    "            primary_language = sluglist_divs[1].find('a', class_='text-slug').text\n",
    "            return primary_language\n",
    "        elif(details_headers[2] == \"Primary Language\" or details_headers[2] == \"Language\"):\n",
    "            primary_language = sluglist_divs[2].find('a', class_='text-slug').text\n",
    "            return primary_language\n",
    "        return \"N/A\"\n",
    "    except IndexError:\n",
    "        return \"N/A\"\n",
    "\n",
    "def getGenre(soup):\n",
    "    try:\n",
    "        div_genres = soup.find('div', id='tab-genres')\n",
    "        h3_tag = div_genres.find('h3').span.text if div_genres else None\n",
    "        if(h3_tag == \"Genres\" or h3_tag == \"Genre\"):\n",
    "            sluglist_div = div_genres.find('div', class_='text-sluglist')\n",
    "            genre = sluglist_div.find('a', class_='text-slug').text\n",
    "            return genre\n",
    "        return \"N/A\"\n",
    "    except IndexError:\n",
    "        return \"N/A\"\n",
    "\n",
    "def getMovieTitle(soup):\n",
    "    movie_title_span = soup.find(\"span\", class_=\"name js-widont prettify\")\n",
    "    movie_name = movie_title_span.text.strip() if movie_title_span else \"N/A\"\n",
    "    return movie_name\n",
    "\n",
    "def getDetailsDivs(soup):\n",
    "    details_divs = soup.find('div', id='tab-details')\n",
    "    return details_divs\n",
    "\n",
    "def getDetailsSlugList(details_divs):\n",
    "    sluglist_divs = details_divs.find_all('div', class_='text-sluglist') if details_divs else []\n",
    "    return sluglist_divs\n",
    "\n",
    "def getDetailHeaders(details_divs):\n",
    "    h3_tags = details_divs.find_all('h3') if details_divs else []\n",
    "    details_headers = [h3.span.text for h3 in h3_tags if h3.span]\n",
    "    return details_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLetterBoxdData(driver, movie_urls, start_index = 0, checkpoint_interval = 300):\n",
    "    movie_data = []\n",
    "    for checkpoint_count, url in movie_urls[start_index:].iterrows():\n",
    "        href = url['URL']\n",
    "        soup = getSoup(driver, href)\n",
    "        categorical_data = getCategoricalData(soup)\n",
    "        numerical_data = getNumericalData(soup)\n",
    "        movie_data.append({**categorical_data, **numerical_data})\n",
    "        if checkpoint_count % checkpoint_interval == 0:\n",
    "            saveCheckpoint(movie_data, f'movie_data_checkpoint_{checkpoint_count // checkpoint_interval}.csv')\n",
    "            movie_data = []\n",
    "\n",
    "    saveCheckpoint(movie_data, f'movie_data_checkpoint_last_batch.csv')\n",
    "    driver.quit()\n",
    "    return movie_data\n",
    "\n",
    "def saveCheckpoint(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def getSoup(driver, href):\n",
    "    movie_url = \"https://letterboxd.com\" + href\n",
    "    driver.get(movie_url)    \n",
    "    waitForElements(driver)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def waitForElements(driver):\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2)\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"tab-genres\")))\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"tab-details\")))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'span.average-rating')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li.stat.filmstat-watches')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li.stat.filmstat-lists')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'li.stat.filmstat-likes')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a.all-link.more-link')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'p.text-link.text-footer')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.releaseyear')))\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'span.name.js-widont.prettify')))\n",
    "    except (TimeoutException, NoSuchElementException):\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMovieUrls(): \n",
    "    return pd.read_csv('movie_url_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_urls = getMovieUrls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.page_load_strategy = 'eager'\n",
    "driver = webdriver.Chrome(options=options)\n",
    "movie_data = extractLetterBoxdData(driver, movie_urls, 0, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_data_checkpoint_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files]\n\u001b[0;32m      4\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mletterbxd_scrapped.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "csv_files = glob.glob('movie_data_checkpoint_*.csv')\n",
    "\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('letterbxd_scrapped.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
